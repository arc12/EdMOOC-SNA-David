require(RCurl)
base_url<-"https://hermes.gwu.edu/cgi-bin/wa?A1=ind9709&L=cybcom'"
base_html<-getURLContent(base_url)
fix(base_html)
base_html
base_html[1]
base_html[[1]]
base_url<-"https://hermes.gwu.edu/cgi-bin/wa?A1=ind9709&L=cybcom'"
base_html<-getURLContent(base_url)
doc <- htmlParse(base_html[[1]])
links <- xpathSApply(doc, "//a/@href")
base_html[1]
base_url<-"https://hermes.gwu.edu/cgi-bin/wa?A1=ind9709&L=cybcom"
base_html<-getURLContent(base_url)
base_html[1]
doc <- htmlParse(base_html[[1]])
links <- xpathSApply(doc, "//a/@href")
fix(links)
links <- xpathSApply(doc, "//a//name/@href")
links <- xpathSApply(doc, "//a/name/@href")
links <- xpathSApply(doc, "//a/@name/@href")
fix(doc)
fix(links)
links <- xpathSApply(doc, "//a/@name")
fix(links)
links[1
]
links[2]
links <- xpathSApply(doc, "//a//@name/href")
links <- xpathSApply(doc, "//a/@name/",xmlGetAttr,'href')
links <- xpathSApply(doc, "//a/@name/")
links <- xpathSApply(doc, "//a/@name")
links <- xpathSApply(doc, "//a/@name",xmlGetAttr,'href')
links <- xpathSApply(doc, "//a/@name",xmlGetAttr,'href')
links <- xpathSApply(doc, "//a[contains(@name)]/@href")
links <- xpathSApply(doc, "//a/@href")
links <- xpathSApply(doc, "//a[contains(@name,'')]/@href")
fix(links)
links[1]
links <- xpathSApply(doc, "//a[contains(@name,'1')]/@href")
links <- xpathSApply(doc, "//a[contains(@name,'12')]/@href")
fix(links)
links <- xpathSApply(doc, "//a[contains(@name,'12')]")
fix(links)
links[1]
links <- xpathSApply(doc, "//a[contains(@name,'')]")
fix(links)
links<-xpathSApply(doc, "//a[contains(@name,'')]")
fix(links)
links[1]
links<-xpathSApply(doc, "//a[@name]")
link[1]
links[1]
#grab all the threads from a page on the cybenetics list
require(RCurl)
base_url<-"https://hermes.gwu.edu/cgi-bin/wa?A1=ind9709&L=cybcom"
base_html<-getURLContent(base_url)
doc <- htmlParse(base_html[[1]])
threads<-xpathSApply(doc, "//a[@name]")
#grab all the threads from a page on the cybenetics list
require(RCurl)
base_url<-"https://hermes.gwu.edu/cgi-bin/wa?A1=ind9709&L=cybcom"
base_html<-getURLContent(base_url)
doc <- htmlParse(base_html[[1]])
threadlist<-xpathSApply(doc, "//a[@name]")
threads <- htmlParse(threadlist[[1]])
threads <- xpathSApply(doc, "//a/href")
threads <- xpathSApply(doc, "//a/@href")
threadlist<-xpathSApply(doc, "//a/@name")
fix(threadlist)
threadlist<-xpathSApply(doc, "//a[@name]")
threads <- xpathSApply(threadlist, "//a/@href")
threads <- xpathSApply(threadlist[1], "//a/@href")
threadlist[1]
base_html<-getURLContent(threadlist[1])
threadlist<-xpathSApply(doc, "//a[@name]")
fix(threadlist)
#grab all the threads from a page on the cybenetics list
require(RCurl)
base_url<-"https://hermes.gwu.edu/cgi-bin/wa?A1=ind9709&L=cybcom"
base_html<-getURLContent(base_url)
doc <- htmlParse(base_html[[1]])
threadlist<-xpathSApply(doc, "//a[@name]")
#loop through threads for links
threads <- xpathSApply(threadlist[1], "//a/@href")
base_html<-getURLContent(threadlist[1])
library(stringr)
matched <- str_match_all(threads[1], "<a href=\"(.*?)\"")
matched <- str_match_all(threadlist[1], "<a href=\"(.*?)\"")
matched <- str_match_all(as.character(threadlist[1]), "<a href=\"(.*?)\"")
fix(matched)
matched
test<-as.character(threadlist[1])
test
threads <- xpathSApply(threadlist[1], "//a/@href", stringsAsFactors=FALSE)
test<-as.character(threadlist[1], stringsAsFactors=FALSE)
ldply(xmlToList(threads), data.frame)
library(plyr)
ldply(xmlToList(threads), data.frame)
ldply(xmlToList(threadlist), data.frame)
ldply(xmlToList(doc), data.frame)
ldply(threadlist[1](doc), data.frame)
threads <- xpathSApply(threadlist[1], "//a/@href")
threads <- xpathSApply(threadlist[1][1], "//a/@href")
threads <- xpathApply(threadlist[1], "//a/@href")
threadlist<-xpathApply(doc, "//a[@name]")
fix(threadlist)
threadlist<-xpathSApply(doc, "//a[@name]")
#loop through threads for links
threads <- xpathSApply(threadlist[1], "//a/@href")
threads <- xpathSApply(threadlist, "//a/@href")
threadlist<-xpathApply(doc, "//a[@name]")
threads <- xpathSApply(threadlist, "//a/@href")
#grab all the threads from a page on the cybenetics list
threads <- xpathSApply(threadlist[1], "//a/@href")
threadlist[1]
threadlist[1][[1]]
threads <- xpathSApply(threadlist[1][[1]], "//a/@href")
fix(threads)
fix(threads)
threadlist<-xpathApply(doc, "//a[@name]")
fix(threadlist)
threadlist[1][[1]]
fix(threadlist)
threadlist
threadlist[[1]
]
threadlist[[2]]
threads <- xpathSApply(threadlist[[1]], "//a/@href")
test<-as.character(threadlist[[1]], stringsAsFactors=FALSE)
test<-as.character(threadlist[[1]])
xmlValue[[threadlist[[1]]]]
xmlValue[[threadlist[[1]]]
]
test<-as.character(threadlist[1])
]
xmlValue(threadlist[[1]])
base_html<-getURLContent(threadlist[[1])
test<-as.character(threadlist[[1]])
xmlValue(threadlist[[1]])
links <- xpathSApply(doc, "//a[@name]//a/@href")
fix(links)
fix(links)
links[1]
links[1][2]
links[1][1]
links[1][[1]]
foeach(link=links) %do% {
link[link][[1]]
}
library(foreach)
foeach(link=links) %do% {
link[link][[1]]
}
foreach(link=links) %do% {
link[link][[1]]
}
link[1][[1]]
}
foreach(link=links) %do% {
link[1][[1]]
}
foreach(link=links) %do% {
link[1][[1]]
base_url<- paste("https://hermes.gwu.edu", link[1][[1]])
base_url
base_html<-getURLContent(base_url)
doc <- htmlParse(base_html[[1]])
}
foreach(link=links) %do% {
link[1][[1]]
base_url<- paste("https://hermes.gwu.edu", link[1][[1]])
base_url
#base_html<-getURLContent(base_url)
# doc <- htmlParse(base_html[[1]])
}
foreach(link=links) %do% {
link[1][[1]]
base_url<- paste("https://hermes.gwu.edu", link[1][[1]], sep = '')
base_url
#base_html<-getURLContent(base_url)
# doc <- htmlParse(base_html[[1]])
}
library('twitteR')
detach("package:ROAuth", unload=TRUE)
library('twitteR')
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-1.1")
library("rj", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='ed_aiplanfor', host='localhost')
source('~/Desktop/MOOC Work/R Workspace/mysql-read.R')
View(data.frame)
source('~/Desktop/MOOC Work/R Workspace/heatmap.R')
library("tm")
doc <- Corpus(VectorSource(data.frame$store))
View(data.frame)
doc <- Corpus(VectorSource(data.frame$post_text))
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument) #TO STEM OR NOT TO STEM
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
findFreqTerms(TDM, 40)
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.9)
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
source('~/Desktop/MOOC Work/R Workspace/mysql-read.R')
library("tm")
doc <- Corpus(VectorSource(data.frame$post_text))
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument) #TO STEM OR NOT TO STEM
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
findFreqTerms(TDM, 40)
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.9)
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
View(data.frame)
source('~/Desktop/MOOC Work/R Workspace/mysql-read.R')
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.8)
WHERE course_grades.normal_grade > 70
source('~/Desktop/MOOC Work/R Workspace/mysql-read.R')
source('~/.active-rstudio-document')
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument) #TO STEM OR NOT TO STEM
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
findFreqTerms(TDM, 40)
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.8)
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
source('~/Desktop/MOOC Work/R Workspace/mysql-read.R')
View(TDM.dense)
View(data.frame)
source('~/.active-rstudio-document')
View(data.frame)
WHERE course_grades.normal_grade = "100"
source('~/Desktop/MOOC Work/R Workspace/mysql-read.R')
source('~/Desktop/MOOC Work/R Workspace/heatmap.R')
source('~/.active-rstudio-document')
library("tm")
doc <- Corpus(VectorSource(data.frame$post_text))
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument) #TO STEM OR NOT TO STEM
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
findFreqTerms(TDM, 40)
findAssocs(TDM, "think", 0.9)
TDM.common = removeSparseTerms(TDM, 0.85)
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
## ****************************************************
## Created: David Sherlock, Cetis, Oct 2013. Based on work by Adam Cooper
## This source code was produced for The University of
## Edinburgh DEI as part of their MOOC initiative.
## Computes some basic statistics about forum use.
## ****************************************************
## Chunks intended for use with knitter in the Rmd file.
## Can be used independently of knitr but NB that some config parameters are set in the Rmd
## @knitr INIT
#establish connection to MySQL, loading library. contains coursera DB exports from 2013
source("../dbConnect.R")
#helper functions
source("../helpers.R")
setwd("~/Desktop/EdMOOC-SNA-master/Text Mining Stats")
Basic Forum Stats
========================================================
```{r echo=FALSE, results='hide'}
courseIDs<-c("aiplan","astro","crit","edc","equine","intro")
echo.sql<-TRUE # echo SQL statements
source("../dbConnect.R")
#helper functions
source("../helpers.R")
## For all SQL here. The wildcard ** is for replacement by ed__equine etc.
#Do we want to include the welcome forums? Should we put a flag here?
## @knitr FORUM_COUNT
forumCount.sql <- "SELECT count(id) forums, sum(deleted) forums_deleted, sum(can_post) forums_can_post,
(SELECT count(1) from **for.forum_subscribe_forums) forums_user_subscriptions
from **for.forum_forums"
threadCount.sql <- "SELECT count(id) threads, sum(deleted) threads_deleted, sum(is_spam) threads_spam,
sum(stickied) threads_stickied, sum(instructor_replied) threads_instructor_replied,
sum(anonymous) threads_anonymous, avg(votes) threads_average_votes,
(SELECT count(1) from **for.forum_tags_threads) threads_tags,
(SELECT count(1) from **for.forum_subscribe_threads) threads_user_subscriptions
from **for.forum_threads"
postCount.sql <- "SELECT count(id) posts, sum(deleted) posts_deleted, sum(is_spam) posts_spam,
sum(stickied) posts_stickied, sum(approved) posts_approved, sum(anonymous) posts_anonymous,
avg(votes) posts_average_votes from **for.forum_posts"
commentCount.sql<-"SELECT count(id) comments, sum(deleted) comments_deleted,
sum(is_spam) comments_spam, sum(anonymous) comments_anonymous, avg(votes) comments_average_votes
from **for.forum_comments"
forumCount.df<-tabular.SELECT(db, courseIDs, forumCount.sql, echo=echo.sql)
threadCount.df<-tabular.SELECT(db, courseIDs, threadCount.sql, echo=echo.sql)
postCount.df<-tabular.SELECT(db, courseIDs, postCount.sql, echo=echo.sql)
commentCount.df<-tabular.SELECT(db, courseIDs, commentCount.sql, echo=echo.sql)
## @knitr POST_ANALYSIS
forum_posts.sql <- "SELECT post_text, post_time from ed_aiplanfor.forum_posts"
forum_posts.df<-list.SELECT(db, courseIDs, forumCount.sql, echo=echo.sql)
## @knitr PREPARE_TEXT
require(tm)
corpus.list<-list()
dtm.list <-list()
for(i in 1:length(courseIDs)){
corp[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corp[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
skipWords <- function(x) removeWords(x, stopwords("english"))
#funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
#corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corp[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
# skipWords <- function(x) removeWords(x, stopwords("english"))
#funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
#corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corpus.list<[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
# skipWords <- function(x) removeWords(x, stopwords("english"))
#funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
#corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corpus.list<[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
for(i in 1:length(courseIDs)){
corpus.list<-[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
for(i in 1:length(courseIDs)){
corpus.list[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
#skipWords <- function(x) removeWords(x, stopwords("english"))
#funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
#corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
forum_posts.df[[1]]$post_text
forum_posts.df[[2]]$post_text
forum_posts.df[[2]]
forum_posts.sql <- "SELECT post_text, post_time from ed_aiplanfor.forum_posts"
forum_posts.df<-list.SELECT(db, courseIDs, forumCount.sql, echo=echo.sql)
## @knitr PREPARE_TEXT
require(tm)
corpus.list<-list()
dtm.list <-list()
for(i in 1:length(courseIDs)){
corpus.list[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
#skipWords <- function(x) removeWords(x, stopwords("english"))
#funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
#corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
forum_posts.df<-list.SELECT(db, courseIDs, forum_posts.sql, echo=echo.sql)
for(i in 1:length(courseIDs)){
corpus.list[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
#skipWords <- function(x) removeWords(x, stopwords("english"))
#funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
#corp[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corpus.list[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
corpus.list[[courseIDs[i]]] <- tm_map(corp, FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corpus.list[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
corpus.list[[courseIDs[i]]] <- tm_map(corpus.list[[courseIDs[i]]] , FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
corpus.list[[courseIDs[i]]] <- Corpus(VectorSource(forum_posts.df[[i]]$post_text))
skipWords <- function(x) removeWords(x, stopwords("english"))
funcs <- list(tolower, removePunctuation, removeNumbers, stripWhitespace, skipWords)
corpus.list[[courseIDs[i]]] <- tm_map(corpus.list[[courseIDs[i]]] , FUN = tm_reduce, tmFuns = funcs)
}
for(i in 1:length(courseIDs)){
dtm.list <- DocumentTermMatrix(corp, control =
# limit word lengths
list(wordLengths = c(2,10))) # ,
}
for(i in 1:length(courseIDs)){
dtm.list <- DocumentTermMatrix(corpus.list[[courseIDs[i]]], control =
# limit word lengths
list(wordLengths = c(2,10))) # ,
}
source('~/Desktop/EdMOOC-SNA-master/Text Mining Stats/Basic Mining Stats.R')
require(mallet)
documents.list[[courseIDs[i]]] <- data.frame(text = forum_posts.df[[i]]$post_text,
stringsAsFactors=FALSE)
mallet.instances <- mallet.import(documents$text, "/Users/David/Desktop/Old Ed/MOOC Work/R Workspace/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
documents.list[[courseIDs[i]]] <- data.frame(text = forum_posts.df[[i]]$post_text, time = forum_posts.df[[i]]$post_time, stringsAsFactors=FALSE)
source('~/Desktop/EdMOOC-SNA-master/Text Mining Stats/Basic Mining Stats.R')
source('~/.active-rstudio-document')
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "/Users/David/Desktop/Old Ed/MOOC Work/R Workspace/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "/Users/David/Desktop/Old Ed/MOOC Work/R Workspace/stoplists/en.txt"")
## Create a topic trainer object.
""
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "/Users/David/Desktop/Old Ed/MOOC Work/R Workspace/stoplists/en.txt")
forum_posts.df[[1]]$post_text
documents.list[[courseIDs[1]]]$time
documents.list[[courseIDs[i]]]$text
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "/Users/David/Desktop/Old Ed/MOOC Work/R Workspace/stoplists/en.txt")
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "/Users/David/Desktop/Old Ed/MOOC Work/R Workspace/stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "en.txt" }")
# limit word lengths
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "en.txt"")
""
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$time, documents.list[[courseIDs[i]]]$text, "en.txt" )
mallet.instances <- mallet.import(documents.list[[courseIDs[i]]]$text, documents.list[[courseIDs[i]]]$text, "en.txt" )
## Create a topic trainer object.
n.topics <- 30
topic.model <- MalletLDA(n.topics)
## Load our documents. We could also pass in the filename of a
