base_html<-getURLContent(base_url)
doc <- htmlParse(base_html[[1]])
}
foreach(link=links) %do% {
link[1][[1]]
base_url<- paste("https://hermes.gwu.edu", link[1][[1]])
base_url
#base_html<-getURLContent(base_url)
# doc <- htmlParse(base_html[[1]])
}
foreach(link=links) %do% {
link[1][[1]]
base_url<- paste("https://hermes.gwu.edu", link[1][[1]], sep = '')
base_url
#base_html<-getURLContent(base_url)
# doc <- htmlParse(base_html[[1]])
}
library('twitteR')
detach("package:ROAuth", unload=TRUE)
library('twitteR')
install.packages(c("rj", "rj.gd"), repos="http://download.walware.de/rj-1.1")
library("rj", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
setwd("~/Desktop/MOOC Work/R Workspace")
save.image("~/Desktop/MOOC Work/R Workspace/workspace.RData")
save.image("~/Desktop/MOOC Work/R Workspace/workspace.RData")
df<-read.csv("eq-forum_connects.csv" , header=T, sep=",")
M = as.matrix( table(df) )
Mrow = M %*% t(M)
#Mcol = t(M) %*% M
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
library("igraph", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
Mrow = M %*% t(M)
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
df<-read.csv("aoiplan_forum_comments.csv" , header=T, sep=",")
M = as.matrix( table(df) )
Mrow = M %*% t(M)
View(df)
#Mcol = t(M) %*% M
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
df<-read.csv("aoiplan_forum_comments_rev.csv" , header=T, sep=",")
df<-read.csv("aoiplan_forum_comments_rev.csv" , header=T, sep=",")
M = as.matrix( table(df) )
Mrow = M %*% t(M)
#Mcol = t(M) %*% M
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
df<-read.csv("aoiplan_forum_posts_rev.csv" , header=T, sep=",")
M = as.matrix( table(df) )
Mrow = M %*% t(M)
#Mcol = t(M) %*% M
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='airplangen', host='localhost')
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='airplangen', host='localhost')
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('SELECT *  from users WHERE last_access_ip != ""')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
#3If you use a self contained MAMP style thing you will need something along the lines of ln -s /Applications/MAMP/tmp/mysql/mysql.sock /tmp/mysql.sock
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('SELECT *  from users WHERE last_access_ip != ""')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
dbDisconnect(mydb)
View(data.frame)
for(i in df){
}
fix(i)
for(i in df){
print "yes"
}
for(i in df){
print "yes"
}
print
print("yes")
for(i in df){
print("yes")
}
curl
library("RCurl")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
curl
library("RCurl")
curl ipinfo.io/8.8.8.8/org
html <- getURLContent(IPLOOKUP)
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/org"
html <- getURLContent(IPLOOKUP)
fix(html)
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/city"
JSON <- getURLContent(IPLOOKUP)
fix(JSON)
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
JSON <- getURLContent(IPLOOKUP)
fix(JSON)
JSON
library("json")
library("rjson")
json <- fromJSON(getURL(IPLOOKUP))
fix(JSON)
fix(JSON)
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
fix(json)
json
json$loc
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('SELECT *  from users WHERE last_access_ip != ""')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
View(data.frame)
data_frame$last_access_ip
data,frame$last_access_ip
data.frame$last_access_ip
fun <- function(x){
x$last_access_ip
}
apply(data.frame, fun)
apply(fun(dataframe))
apply(fun,data.frame)
fix(fun)
fun <- function(x){
x$last_access_ip
}
apply(data.frane,fun)
function1 <- function(x){
x$last_access_ip
}
apply(data.frane,function1)
lapply(data.frane,function1)
lapply(data.frame,function1)
apply(data.frame,1,min)
View(data.frame)
apply(data.frame[7],min)
apply(data.frame[7],function1)
apply(data.frame[7],function1)
lapply(data.frame[4:9], function1)
function1 <- function(x){
x
}
lapply(data.frame[4:9], function1)
lapply(data.frame[7], function1)
function1 <- function(x){
x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "test" WHERE last_access_ip ="',x ,'" ;')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
lapply(data.frame[7], function1)
function1 <- function(x){
x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "test" WHERE last_access_ip ="',x ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
lapply(data.frame[7], function1)
function1 <- function(x){
x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "test" WHERE last_access_ip ="',x ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('SELECT *  from users WHERE last_access_ip != ""')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
lapply(data.frame[7], function1)
lapply(data.frame[7], function1)
function1 <- function(x){
x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "crap" WHERE last_access_ip ="186.69.132.131" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
lapply(data.frame[7], function1)
function1 <- function(x){
x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "crap" WHERE last_access_ip ="',x ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
function1 <- function(x){
x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "test" WHERE last_access_ip ="',x ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
lapply(data.frame[7], function1)
function1 <- function(x){
tes<-x
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "test" WHERE last_access_ip ="',x ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
lapply(data.frame[7], function1)
for(i in data.frame[7]){
print("yes")
}
}
data.frame[7]
i
for(i in data.frame[7]){
i
}
for(i in data.frame[7]){
i
}
IPLOOKUP <- paste("http://ipinfo.io/",i,/"json")
IPLOOKUP <- "http://ipinfo.io/8.8.8.8/json"
json <- fromJSON(getURL(IPLOOKUP))
json'8'
json[8]
json[7]
for(i in data.frame[7]){
IPLOOKUP <- paste("http://ipinfo.io/",i,/"json")
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "',json[7] ,'" WHERE last_access_ip ="',i ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
for(i in data.frame[7]){
IPLOOKUP <- paste("http://ipinfo.io/",i,/"json")
IPLOOKUP <- paste("http://ipinfo.io/",i,"/json")
for(i in data.frame[7]){
IPLOOKUP <- paste("http://ipinfo.io/",i,"/json")
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "',json[7] ,'" WHERE last_access_ip ="',i ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
function1 <- function(x){
tes<-x
IPLOOKUP <- paste("http://ipinfo.io/",x,"/json")
json <- fromJSON(getURL(IPLOOKUP))
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='aiplangen', host='localhost')
query<-paste('update users set last_accessed_city = "test" WHERE last_access_ip ="',x ,'" ;')
query
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
}
lapply(data.frame[7], function1)
browser(lapply(data.frame[7], function1))
browse()
browse(function1)
rowser(lapply(data.frame[7], function1))
browser(lapply(data.frame[7], function1))
function1()
lapply(data.frame[7], function1)
debug
debug()
debug(lapply(data.frame[7], function1))
options(error = browser)
debug(lapply(data.frame[7], function1))
lapply(data.frame[7], function1)
input<- read.csv("country.csv")
View(input)
input<- read.csv("country.csv")
View(input)
input$Country
View(input)
Map<- data.frame(input$Country, input$Number)
names(Map)<- c("Country", "Number")
Geo=gvisGeoMap(Map, locationvar="Country", numvar="Number",
options=list(height=350, dataMode='regions'))
plot(Geo)
library(googleVis)
exit
library(googleVis)
install.packages("googleVis")
input<- read.csv("country.csv")
Map<- data.frame(input$Country, input$Number)
names(Map)<- c("Country", "Number")
Geo=gvisGeoMap(Map, locationvar="Country", numvar="Number",
options=list(height=350, dataMode='regions'))
library("googleVis", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
Geo=gvisGeoMap(Map, locationvar="Country", numvar="Number",
options=list(height=350, dataMode='regions'))
plot(Geo)
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
query<-paste('SELECT post_text as store from forum_posts)
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
:
)
s
q
{}
)
;
z
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
query<-paste('SELECT post_text as store from forum_posts)
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
dbDisconnect(mydb)
dbDisconnect(mydb)
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
library("RMySQL")
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
query<-paste('SELECT post_text as store from forum_posts)
data.frame = dbGetQuery(mydb,query)
''
'
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='paddytherabbit', host='localhost')
query<-paste('SELECT post_text as store from forum_posts')
data.frame = dbGetQuery(mydb,query)
mydb = dbConnect(MySQL(), user='KingKongRoot', password='CarBoot', dbname='ed_aiplanfor', host='localhost')
query<-paste('SELECT post_text as store from forum_posts')
data.frame = dbGetQuery(mydb,query)
dbDisconnect(mydb)
dbDisconnect(mydb)
View(data.frame)
doc <- Corpus(VectorSource(data.frame$store))
library("tm")
doc <- Corpus(VectorSource(data.frame$store))
summary(doc)
doc.corpus <-tm_map(doc, removeWords, stopwords("english")) #removes stopwords
doc.corpus <-tm_map(doc.corpus, stripWhitespace) #removes stopwords
doc.corpus <-tm_map(doc.corpus, tolower)
doc.corpus <-tm_map(doc.corpus, removeNumbers)
doc.corpus <-tm_map(doc.corpus, removePunctuation)
library(SnowballC)
doc.corpus <- tm_map(doc.corpus, stemDocument) #TO STEM OR NOT TO STEM
TDM <- TermDocumentMatrix(doc.corpus)
TDM
inspect(TDM[1:10,1:10])
findFreqTerms(TDM, 40)
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.8)
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
fix(TDM.common)
TDM.common
findFreqTerms(TDM, 40)
TDM.common = removeSparseTerms(TDM, 0.5)
TDM.common
TDM.common = removeSparseTerms(TDM, 1)
TDM.common = removeSparseTerms(TDM, 0.99)
TDM.common
dim(TDM)
dim(TDM.common)
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
findAssocs(TDM, "think", 0.8)
TDM.common = removeSparseTerms(TDM, 0.9)
dim(TDM)
dim(TDM.common)
TDM.common
library(slam)
TDM.dense <- as.matrix(TDM.common)
TDM.dense
object.size(TDM.common)
object.size(TDM.dense)
library(reshape2)
TDM.dense = melt(TDM.dense, value.name = "count")
head(TDM.dense)
library(ggplot2)
ggplot(TDM.dense, aes(x = Docs, y = Terms, fill = count)) +
geom_tile(colour = "white") +
scale_fill_gradient(high="#FF0000" , low="#FFFFFF")+
ylab("") +
theme(panel.background = element_blank()) +
theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())
documents <- data.frame( text = data.frame$store, stringsAsFactors=FALSE)
documents$id <- Sys.time() + 30 * (seq_len(nrow(documents))-1)
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 5
topic.model <- MalletLDA(n.topics)
#loading the documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
require(mallet)
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
mallet.instances <- mallet.import( documents$text ,  documents$text , "stoplists/en.txt", token.regexp = "\\p{L}[\\p{L}\\p{P}]+\\p{L}")
## Create a topic trainer object.
n.topics <- 5
topic.model <- MalletLDA(n.topics)
#loading the documents
topic.model$loadDocuments(mallet.instances)
## Get the vocabulary, and some statistics about word frequencies.
##  These may be useful in further curating the stopword list.
vocabulary <- topic.model$getVocabulary()
word.freqs <- mallet.word.freqs(topic.model)
## Optimize hyperparameters every 20 iterations,
##  after 50 burn-in iterations.
topic.model$setAlphaOptimization(20, 50)
## Now train a model. Note that hyperparameter optimization is on, by default.
##  We can specify the number of iterations. Here we'll use a large-ish round number.
topic.model$train(200)
## NEW: run through a few iterations where we pick the best topic for each token,
##  rather than sampling from the posterior distribution.
topic.model$maximize(10)
## Get the probability of topics in documents and the probability of words in topics.
## By default, these functions return raw word counts. Here we want probabilities,
##  so we normalize, and add "smoothing" so that nothing has exactly 0 probability.
doc.topics <- mallet.doc.topics(topic.model, smoothed=T, normalized=T)
topic.words <- mallet.topic.words(topic.model, smoothed=T, normalized=T)
# from http://www.cs.princeton.edu/~mimno/R/clustertrees.R
## transpose and normalize the doc topics
topic.docs <- t(doc.topics)
topic.docs <- topic.docs / rowSums(topic.docs)
## Get a vector containing short names for the topics
topics.labels <- rep("", n.topics)
for (topic in 1:n.topics) topics.labels[topic] <- paste(mallet.top.words(topic.model, topic.words[topic,], num.top.words=5)$words, collapse=" ")
# have a look at keywords for each topic
topicsforsubreddit<-topics.labels
# create data.frame with columns as authors and rows as topics
topic_docs <- data.frame(topic.docs)
names(topic_docs) <- documents$id
topics.labels
View(documents)
source('~/Desktop/MOOC Work/R Workspace/topic_moddeling.R')
topics.labels
n.topics <- 50
source('~/Desktop/MOOC Work/R Workspace/topic_moddeling.R')
topics.labels
fix(topics.labels)
df<-read.csv("topic_posts.csv" , header=T, sep=",")
df<-read.csv("topic_posts.csv" , header=T, sep=",")
save.image("~/Desktop/MOOC Work/Rsession.RData")
df<-read.csv("topic_posts.csv" , header=T, sep=",")
df<-read.csv("country.csv" , header=T, sep=",")
df<-read.csv("topics_posts.csv" , header=T, sep=",")
M = as.matrix( table(df) )
Mrow = M %*% t(M)
#Mcol = t(M) %*% M
iMrow = graph.adjacency(Mrow, mode = "undirected")
E(iMrow)$weight <- count.multiple(iMrow)
iMrow <- simplify(iMrow)
write.graph(iMrow, file="graph.graphml", format="graphml");
